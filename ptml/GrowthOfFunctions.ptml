<html>
    <head>
        <link href="style.css" rel="stylesheet" type="text/css"/>
        <title>
            Design and Analysis of Algorithms: Growth of Functions
        </title>
    </head>
    <body>
<!--include menu.txt -->
        <h1>
            Design and Analysis of Algorithms: Growth of Functions
        </h1>


    <details>
    <summary class="sum1">
    A review of Big-O, summations and algebra. 
    </summary>
        <details>
        <summary class="sum2">
            Different asymptotic notations
        </summary>
            <details>
                <summary class="sum3">
                Big O
                </summary>
                <figure class="left">
                <img src="https://cdn.kastatic.org/ka-cs-algorithms/O_fn.png">
                </figure>
                <table>
                    <tr>
                        <th>
                            Notation
                        </th>
                        <th>
                            Intuition
                        </th>
                        <th>
                            Definition
                        </th>
                    </tr>
                    <tr>
                        <td>
                            <em>g(n) = O(f(n))</em>
                        </td>
                        <td>
                            <em>g</em> is bounded above by <em>f</em>
                            (up to constant factor) asymptotically
                        </td>
                        <td>
                            <em>g(n) &le; k*f(n)</em> for some
                            positive <em>k</em>,
                            after some <em>n</em>
                        </td>
                    </tr>
                </table>
            </details>

            <details>
                <summary class="sum3">
                Big &Theta;
                </summary>
                <figure class="left">
                <img src="https://cdn.kastatic.org/ka-cs-algorithms/theta_fn.png">
                </figure>
                <table>
                    <tr>
                        <th>
                            Notation
                        </th>
                        <th>
                            Intuition
                        </th>
                        <th>
                            Definition
                        </th>
                    </tr>
                    <tr>
                        <td>
                            <em>g(n) = &Theta;(f(n))</em>
                        </td>
                        <td>
                            <em>g</em> is bounded above and below
                            by <em>f</em> asymptotically
                        </td>
                        <td>
                            <em>k<sub>2</sub>*f(n) &le;
                            g(n) &le; k<sub>2</sub>*f(n)</em>
                            for some positive
                            <em>k<sub>1</sub></em>,
                                <em>k<sub>2</sub></em>
                        </td>
                    </tr>
                </table>
            </details>
            <details>
                <summary class="sum3">
                Big &Omega;
                </summary>
                <figure class="left">
                <img src="https://cdn.kastatic.org/ka-cs-algorithms/Omega_fn.png">
                </figure>
                <table>
                    <tr>
                        <th>
                            Notation
                        </th>
                        <th>
                            Intuition
                        </th>
                        <th>
                            Definition
                        </th>
                    </tr>
                    <tr>
                        <td>
                            <em>g(n) = &Omega;(f(n))</em>
                        </td>
                        <td>
                            <em>g</em> is bounded below by <em>f</em>
                            asymptotically
                        </td>
                        <td>
                            <em>g(n) &ge; k*f(n)</em>
                            for some positive <em>k</em>
                        </td>
                    </tr>
                </table>
            </details>
            <details>
                <summary class="sum3">
                Little o
                </summary>
                <table>
                    <tr>
                        <th>
                            Notation
                        </th>
                        <th>
                            Intuition
                        </th>
                        <th>
                            Definition
                        </th>
                    </tr>
                    <tr>
                        <td>
                            <em>g(n) = o(f(n))</em>
                        </td>
                        <td>
                            <em>g</em> is dominated by <em>f</em>
                            asymptotically
                        </td>
                        <td>
                            <em>g(n) &lt; k*f(n)</em> for
                            every positive <em>k</em>
                        </td>
                    </tr>
                </table>
            </details>

            <details>
                <summary class="sum3">
                Little &omega;
                </summary>
                <table>
                    <tr>
                        <th>
                            Notation
                        </th>
                        <th>
                            Intuition
                        </th>
                        <th>
                            Definition
                        </th>
                    </tr>
                    <tr>
                        <td>
                            <em>g(n) = &omega;(f(n))</em>
                        </td>
                        <td>
                            <em>g</em> dominates <em>f</em>
                            asymptotically
                        </td>
                        <td>
                            <em>g(n) &gt; k*f(n)</em> for
                            every positive <em>k</em>
                        </td>
                    </tr>
                </table>
        </details>
    </details> 
    
    <details>
    <summary class="sum2">
    Why have different notations?
    </summary> 
        <p>
        Why not just Big-&Theta;?
        </p>
        <p>
        From the Khan Academy site:<br>
        <blockquote>
        We use big-&Theta; notation to asymptotically bound the 
        growth of a running time to
        within constant factors above and below. 
        Sometimes we want to bound from only
        above. For example, although the worst-case 
        running time of binary search is &Theta;(lg n),
        it would be incorrect to say that binary search runs in 
        &Theta;(lg n) time in all cases.
        What if we find the target value upon the first
        guess? Then it runs in &Theta;(1) time. 
        The running time of binary search is never worse than 
        &Theta;(lg n), but it's sometimes better. 
        It would be convenient to have a form of
        asymptotic notation that means 
        "the running time grows at most this much, but
        it could grow more slowly."
        We use "big-O" notation for just such occasions.
        </blockquote>
        </p>


    </details> 

    <details>
    <summary class="sum2">
    Highest-order term
    </summary>
        <p>
            Graphs and a table on why we only care about the highest-order
        </p>
        <p>
          <img
          src="https://cdn.kastatic.org/ka-cs-algorithms/6n2_vs_100n%2B300.png"
            height="200" width="300">
          <br>
          <br>
          <img
          src="https://cdn.kastatic.org/ka-cs-algorithms/0.6n2_vs_1000n%2B3000.png"
            height="200" width="300">
        </p>
        <table>
            <tr>
                <th>
                    <em>n</em>
                </th>
                <th>
                    Fast Alpha machine,
                    <br />
                    C code,
                    <br />
                    running O(n<sup>3</sup>) algo.
                </th>
                <th>
                    TRS-80
                    <br />
                    interpreted Basic,
                    <br />
                    running O(n) algo.
                </th>
            </tr>
            <tr>
                <td>
                    10
                </td>
                <td>
                    .6 microsecs.
                </td>
                <td>
                    200 millisecs.
                </td>
            </tr>
            <tr>
                <td>
                    100
                </td>
                <td>
                    .6 millisecs.
                </td>
                <td>
                    2 secs.
                </td>
            </tr>
            <tr>
                <td>
                    1000
                </td>
                <td>
                    .6 secs.
                </td>
                <td>
                    20 secs.
                </td>
            </tr>
            <tr>
                <td>
                    10,000
                </td>
                <td>
                    10 mins.
                </td>
                <td>
                    3.2 mins.
                </td>
            </tr>
            <tr>
                <td>
                    100,000
                </td>
                <td>
                    7 days
                </td>
                <td>
                    32 mins.
                </td>
            </tr>
            <tr>
                <td>
                    1,000,000
                </td>
                <td>
                    19 years
                </td>
                <td>
                    5.4 hours
                </td>
            </tr>
        </table>
        <p>
        (Table from Jon Bentley, <em>Programming Pearls</em>.)
        </p>

    </details> 
    
    <details>
        <summary class="sum2">
        Notes on log functions.
        </summary>
    
        <p>
            The textbook uses lg for base-2 logarithms.
            <b>But</b>... the base doesn't really matter!
            Why?
        </p>

        <table>
            <tr>
                <th>
                    <i>n</i>
                </th>
                <th>
                    log base-2
                </th>
                <th>
                    log base-10
                </th>
                <th>
                    Differ by...
                </th>
            </tr>
            <tr>
                <td>
                    2
                </td>
                <td>
                    1
                </td>
                <td>
                    .301
                </td>
                <td>
                    3.322
                </td>
            </tr>
            <tr>
                <td>
                    10
                </td>
                <td>
                    3.322
                </td>
                <td>
                    1
                </td>
                <td>
                    3.322
                </td>
            </tr>
            <tr>
                <td>
                    32
                </td>
                <td>
                    5
                </td>
                <td>
                    1.505
                </td>
                <td>
                    3.322
                </td>
            </tr>
        </table>

        <p>
        They differ by a constant factor!
        <br />
        <b>Log rule</b>: log<sub><i>b</i></sub>(<i>x</i>)
        = log<sub><i>c</i></sub>(<i>x</i>)
        / log<sub><i>c</i></sub>(<i>b</i>)
        <br />
        So...
        log<sub>10</sub>(32)
        = log<sub>2</sub>(32)
        / log<sub>2</sub>(10)
        <br />
        = 5 / 3.322 = 1.505
        </p>

        <p>
         Also, let's look at logs versus polynomial growth. Even for very small
         powers <i>p</i> &gt; 0, <i>x</i><sub><i>p</i></sub> grows faster than 
         log <i>x</i>. Below we use natural logs:
        </p>

        <table>
            <tr>
                <th>
                    <i>p</i>
                </th>
                <th>
                    <i>x</i><sup><i>p</i></sup> &gt; log <i>x</i> at
                </th>
            </tr>
            <tr>
                <td>
                    .3
                </td>
                <td>
                    379
                </td>
            </tr>
            <tr>
                <td>
                    .2
                </td>
                <td>
                    332,105
                </td>
            </tr>
            <tr>
                <td>
                    .1
                </td>
                <td>
                    3,430,631,121,407,801
                </td>
            </tr>
        </table>

    </details>

    <details>
        <summary class="sum2">
        Meaning of constants in O, &Theta; and &Omega; expressions
        </summary>
        <p>
        Let's say we posit a search operating in <em>2n + 3</em> time.<br>
        We want to show that this has &Theta;(n) complexity.<br>
        We must find <em>k<sub>1</sub></em>, <em>k<sub>2</sub></em>, 
        and <em>n<sub>0</sub></em>, so that:
        <em>k<sub>1</sub> * n &lt; 2n + 3 &lt; k<sub>2</sub> * n</em>
        <br>
        One solution is <em>k<sub>1</sub> = 1</em>,
        <em>k<sub>2</sub> = 3</em>,
        and <em>n<sub>0</sub> = 4</em>.
        </p>
    </details> 

  </details>

    <details>
    <summary class="sum1">
    Algorithmic design in action
    </summary>
  
    <details>
    <summary class="sum2">
    Fibonacci numbers
    </summary> 
        <p>
          The problem of computing, given <em>n &ge; 0</em>,
          the <em>n</em><sup>th</sup> Fibonacci number <em>F<sub>n</sub></em>.
      
        </p>

        <p>
            The Fibonacci discussion is not in the textbook. 
        One place where it is presented in a nice way
        similar to what I will do in class is in
        section 0.2 of the DasGupta, Papadimitriou,
        Vazirani <em>Algorithms</em> book.
        </p>
        
        <p>
        Talked about the obvious recursive algorithm;
        how it is very slow, intuitively due to
        recomputing the same subproblems. 
        Proved that the recursion tree grows at
        least as fast as the Fibonacci sequence itself. 
        </p>
        
        <h4>Sketch of Proof</h4>
        <p>
        Note that every leaf of the recursion tree
        returns one. The sum of the value
        of all of these leaves is going to
        be the Fibonacci number we return.
        Therefore, there must be at least
        as many operations as the Fibonacci number itself.
        </p>
        
        
        <p>
        Showed how to speed the recursive algorithm algorithm up by "memoization":
        Using a table to store answers for subproblems already computed. 
        Analyzed the running time and space of the recursive memoized version and of
        the iterative algorithm that fills the table going in the forward direction. 
        Pointed out how to reduce space to constant.
        </p>
        <p>
        <a
        href="https://github.com/gcallah/algorithms/blob/master/Python/DivideAndConquer/fibonacci.py">
            Naive and faster Fibonacci code.</a>
        </p>
        <p>
        By the way, this is an open-source project: you may contribute!
        </p>
        
        <p>
        There is a closed-form formula for computing Fibonacci numbers. 
        </p>
        <p>
        
        So why can't we claim we can compute <em>n</em>th Fibonacci number in constant time?
        Did not go into details, but this is related to our inability to directly represent
        irrational numbers such as square roots, maybe related to precision/rounding,
        and generally related to what operations are allowed and what are not, in an algorithm. 
        </p>
        <p>
        Using some tricks one can construct <em>n</em>th Fibonacci number
        in <em>O(log n)</em> arithmetic operations, where we count additions/subtractions/multiplications
        and do not worry about the fact that eventually the numbers get too large to manipulate
        in constant time.
        </p>
    </details> 
    
</details>

<details>
<summary class="sum1">
Data structures
</summary>
        <p>
        Data structures form a very important part of algorithmic research. 
        </p>
        
        <p>
        However, this course does not focus on data structures. 
        We will only devote a couple of lectures, total, to this subject. 
        (There are algorithms courses that spend their entire time on data structures. 
        We have an advanced data structures expert in the department, Prof. John Iacono.)
        </p>
        <ul>A quick reminder of basic data structures:
            <li>
                Arrays
            </li>
            <li>
                Linked list
                <br><img
                src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Singly-linked-list.svg/408px-Singly-linked-list.svg.png">
            </li>
            <li>
                Doubly-linked list
                <br>
                <img
                src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Doubly-linked-list.svg/610px-Doubly-linked-list.svg.png">
            </li>
            <li>Stacks
                <br>
                <img
                src="https://upload.wikimedia.org/wikipedia/commons/b/b4/Lifo_stack.png">
            </li>
            <li>
                Queues
                <br>
                <img
                src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Data_Queue.svg/405px-Data_Queue.svg.png">
            </li>
            <li>
                Double-ended queues
            </li>
            <li>
                ADT (<i>abstract data type</i>):
                <br>the set of operations a data structure promises to provide 
                and the rules that they have to obey.
            </li>
        </ul>
        <details>
          <summary class="sum2">
          Detailed case study: Iterables as an ADT. 
          </summary>
            <details>
                 <summary class="sum3">
                 Basic operations. 
                 </summary>
                     <ul>
                         <li>Get iterator.</li>
                         <li>Get next value.</li>
                         <li>Detect end of iterables.</li>
                     </ul>
            </details>
           
            <details>
                 <summary class="sum3">
                       Extended operations. 
                 </summary>
                     <ul>
                         <li>
                         Rewind? We may not be able to if the 
                         iterator is exhausted in iterating
                             through it.
                         </li>
                         <li>
                            Iterate in reverse?
                         </li>
                         <li>
                            Access an arbitrary element? (By index, say.)
                         </li>
                     </ul>
            </details>
            <details>
                 <summary class="sum3">
                   Implementations. 
                 </summary>
                   <ul>
                       <li>
                           Straight-forward: Array.
                       </li>
                       <li>
                           More complex: Python lists: somewhat like an array,
                           but also like a linked list.
                       </li>
                   </ul>
            </details>
        </details>
    </details>

    <details>
        <summary class="sum1">
        Homework
        </summary>
        <ol type="1">

            <li>if f(n) = 3n<sup>2</sup> + n<sup>3</sup>
                    lg n, then f(n) is 
                <ol type="a" class="nested">
                    <li>
                        O(n<sup>2</sup>)
                    </li>
                    <li>
                        O(n<sup>3/2</sup>)
                    </li>
                    <li>
                        O(n<sup>3</sup> lg n)
                    </li>
                    <li>
                        O(n<sup>2/3</sup>)
                    </li>
                </ol>
            </li>

            <li>
                What is the asymptotic relationship between the functions:
                x<sup>p</sup>  and k<sup>x</sup>?
                (Assuming that p &ge; 1 and k &gt; 1 are constants:)
                <ol type="a" class="nested">
                    <li>
                        x<sup>p</sup>
                        is O(k<sup>x</sup>)
                    </li>
                    <li>
                        k<sup>x</sup>
                        is O(x<sup>p</sup>)
                    </li>
                    <li>
                        x
                        is O(k)
                    </li>
                    <li>
                        Both b and c
                    </li>
                </ol>
            </li>

            <li>
                For functions, n<sup>k</sup> and c<sup>n </sup>,
                what is the asymptotic relationship between these
                functions? Assume that k &ge; 1,and c &ge; 1 are constant.
                <ol type="a" class="nested">
                    <li>
                        n<sup>k</sup>
                        is O(c<sup>n</sup>)
                    </li>
                    <li>
                        n<sup>k</sup>
                        is &Omega;(c<sup>n</sup>)
                    </li>
                    <li>
                        n<sup>k</sup>
                        is &Theta;(c<sup>n</sup>)
                    </li>
                    <li>
                        None of the above
                    </li>
                </ol>
            </li>

            <li>
                If f(n) = 5 lg n + 2 lg n! + (n<sup>2</sup> + 1) lg n,
                what is the big-O notation for f(n)?
                <ol type="a" class="nested">
                    <li>n</li>
                    <li>n<sup>2</sup></li>
                    <li>n lg n</li>
                    <li>n<sup>2</sup> lg n</li>
                </ol>
            </li>
            

            <li>
                What is the time complexity for the following piece of code?
                <pre>
                <code>
                 sum = 0;
                 for (int i = 0; i &lt; n; i++)
                     for (j = 1; j &lt; n; j = j * 2)
                         sum += n;
                </code>
                </pre>
                <ol type="a" class="nested">
                    <li>
                        O(n<sup>2</sup>)
                    </li>
                    <li>
                        O(n)
                    </li>
                    <li>
                        O(lg n)
                    </li>
                    <li>
                        O(n lg n)
                    </li>
                </ol>
            </li>

            <li>
                If f(x) = (x<sup>3</sup> &minus; 1) / (5x + 1) then f(x) is
                <ol type="a" class="nested">
                    <li>
                        O(x<sup>2</sup>)
                    </li>
                    <li>
                        O(x)
                    </li>
                    <li>
                        O(x<sup>3</sup>/5)
                    </li>
                    <li>
                        O(1)
                    </li>
                </ol>
            </li>

            <li>The Big-O complexity of 1 + 2 + 3 + 4 ... + n is?
                (Assume we must add the numbers
                one at a time, rather than using Gauss's trick to get a closed
                form for the sum.)
                <ol type="a" class="nested">
                    <li>
                        O(n)
                    </li>
                    <li>
                        O(n<sup>2</sup>)
                    </li>
                    <li>
                        O(3n)
                    </li>
                    <li>
                        O(n<sup>3</sup>)
                    </li>
                </ol>
            </li>

            <li>
                The Big-O complexity of
                1 + 2 + 3 + 4 + ... + 100 is?
                (Assume we must add the numbers
                one at a time, rather than using Gauss's trick to get a closed
                form for the sum.)
                <ol type="a" class="nested">
                    <li>
                        O(1)
                    </li>
                    <li>
                        O(n)
                    </li>
                    <li>
                        O(n<sup>2</sup>)
                    </li>
                    <li>
                        O(3n)
                    </li>
                </ol>
            </li>

            <li>
                What is big O for following code?
                <pre>
                <code>
                    void complex(int n)
                    {
                        int i, j;
                        for(i = 1; i &lt; n; i++) {
                            for(j = 1; j &lt; log(i); j++)
                        }
                        printf("Algorithms");
                    }                        
                </code>
                </pre>
            </li>

            <li>
                What is big O for following code?
                <pre>
                <code>
                    void complex(int n)
                    {
                        int i;
                        for(i = n; i &gt; 1; i = i/2){
                            printf("Algorithms")
                        }
                    }                            
                </code>
                </pre>
            </li>
        </ol>
    </details>


    <a name="note1">* Based on Prof. Boris Aronov's lecture notes. </a>
    <br>
    <a name="note2">** Material drawn from Khan Academy.</a>

    </body>
<!--include google_analytics.txt -->
</html>
